---
# tasks file for k8s_master

# Adding kubernetes repo from base url using gpg keys
- name: install APT Transport HTTPS
  apt:
    name: apt-transport-https
    state: present
 
- name: add Kubernetes apt-key
  apt_key:
    url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    state: present
 
- name: add Kubernetes' APT repository
  apt_repository:
    repo: deb http://apt.kubernetes.io/ kubernetes-xenial main
    state: present
    filename: 'kubernetes'

# Installing 'docker', 'kubeadm' and 'iproute-tc' on master node 
- name: Installing iproute2 & kubeadm on Master Node
  package:
    name:
      - kubeadm
      - iproute2
    state: present

# Starting and enabling the docker on master node.
- name: enabling Docker on Master Node
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: ensure docker registry is available
  apt_repository: repo='deb https://download.docker.com/linux/ubuntu bionic stable' state=present

- name: ensure docker and dependencies are installed
  apt: name=docker-ce update_cache=yes

# Starting kubelet on master node. we maybe have a question on why kubelet is run on master node?
# The reason for running kubelet on master node is that, kubeadm uses containers (pods) to deploy etcd and the api server components. 
# For this, static manifests are created as yaml-files which are picked up by the kubelet on the master node 
# to provide the infrastructure pods. An added benefit is that you have the master node metrics available from the kubernetes api.
- name: Staring kubelet on Master Node
  service:
    name: kubelet
    state: started
    enabled: yes

# Work around for the issue (level=fatal msg="pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService")
- name: Workarund - Delete /etc/containerd/config.toml
  command: rm /etc/containerd/config.toml

# Work around for the issue (level=fatal msg="pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService")
- name: Workarund - Restart containerd
  command: systemctl restart containerd

# Pulling all the images required for k8s master on master node.
- name: Pulling the images of k8s master
  command: kubeadm config images pull

# Updating Docker cgroup on Master Node. Reason for using cgroupdriver as 'systemd' instead of default ones, 
# https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/
- name: Updating Docker cgroup on Master Node
  copy:
    dest: /etc/docker/daemon.json
    content: |
      {
      "exec-opts": ["native.cgroupdriver=systemd"]
      }
# Restart the docker on Master node to get all things activated
- name: Restart docker on Master Node
  service:
    name: docker
    state: restarted

# Set the hostname as FQDN 
- name: Set the hostname as FQDN 
  shell: hostnamectl set-hostname $(curl http://169.254.169.254/latest/meta-data/local-hostname)

# Check hostname
- name: Check hostname
  shell: hostname
  register: hostname_output
- debug:
    var: hostname_output.stdout_lines

# Create kubernetes directory for all kubernetes related files
- name: Create kubernetes directory for all kubernetes related files
  shell: mkdir -p $HOME/kubernetes

# Copy service.yml template to /etc/kubernetes/service.yml file
- name: Copy service.yml template to /etc/kubernetes/aws.yml file
  template:
    dest: /etc/kubernetes/service.yml
    src: roles/k8s_master_ovpn/templates/service.yml.j2

# Copy deployment.yml file for deployment and service running on kubernetes
- name: Create a $HOME/kubernetes/deployment.yml files
  template:
    dest: $HOME/kubernetes/deployment.yml
    src: roles/k8s_master_ovpn/templates/deployment.yml.j2

# Initialize kubernetes cluster with a pod network so that the Pods can talk to each other, also ignoring errors like 'NumCPU', 'Mem'
# and 'FileContent--proc-sys-net-bridge-bridge-nf-call-iptables' as they are limited to system resources.
- name: Initializing k8s cluster
  command: kubeadm init --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem --ignore-preflight-errors=FileContent--proc-sys-net-bridge-bridge-nf-call-iptables

# Setting kubectl on master node with updating kubernetes configuration from default path to '$HOME/.kube/config' path and updating
# ownership permissions to user and group 
- name: Setting up kubectl on Master Node
  shell:
    cmd: |
      mkdir -p $HOME/.kube
      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Deploying a CNI plugin on master node, we use 'calico' for it, https://projectcalico.docs.tigera.io/about/about-calico
- name: Deploying Calico on Master Node
  command: kubectl apply -f https://projectcalico.docs.tigera.io/archive/v3.21/manifests/calico.yaml

# A token creation command for slave nodes. This command is used to get the token for the slave node to join the cluster.
# Using “register” the output of the “command” module is stored in a variable called “token”.
# Now this token variable contain the command that we need to run on slave node, so that it joins the master node.
# - name: Creating token for Slave
#   command: kubeadm token create  --print-join-command
#   register: token

- name: install Ovpn client
  apt:
    name: openvpn
    state: present

- name: install network-manager-openvpn client
  apt:
    name: network-manager-openvpn
    state: present

- name: Start openvpn client service
  command: systemctl start openvpn
 
- name: Enable openvpn client service
  command: systemctl enable openvpn
  
# Cleaning caches on nodes RAM's. The “shell” module used here to clean the buffer cache on the master node,
# because while doing the setup it will create lots of temporary data on RAM
- name: Cleaning Caches on RAM
  shell: echo 3 > /proc/sys/vm/drop_caches