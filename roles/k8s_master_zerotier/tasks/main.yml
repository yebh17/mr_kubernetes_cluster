---
# tasks file for k8s_master

- name: Install net-tools for network management
  apt:
    name: net-tools
    state: present

- name: Install nginx LB for external access
  apt:
    name: nginx
    state: latest
    update_cache: yes

- name: Remove default nginx config
  shell: rm -rf /etc/nginx/nginx.conf

- name: Update apt-get repo and cache
  apt: update_cache=yes force_apt_get=yes

- name: Copy the bash script to our nodes, the script downloads and installs ZeroTier App
  template:
    src: vpn.sh
    dest: "/root/"

- name: Make the vpn script executable
  shell: chmod 755 /root/vpn.sh

- name: Run the vpn script 
  shell: /root/vpn.sh
  register: vpn_setup_master_debug
- debug:
      msg: "{{ vpn_setup_master_debug.stdout }}"

- name: Join the machine to the vpn network
  shell: zerotier-cli join 8bd5124fd64a2f39
  register: join_node_master_debug
- debug:
      msg: "{{ join_node_master_debug.stdout }}"

# Adding kubernetes repo from base url using gpg keys
- name: install APT Transport HTTPS
  apt:
    name: apt-transport-https
    state: present
 
- name: add Kubernetes apt-key
  apt_key:
    url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    state: present
 
- name: add Kubernetes' APT repository
  apt_repository:
    repo: deb http://apt.kubernetes.io/ kubernetes-xenial main
    state: present
    filename: 'kubernetes'

# Installing 'docker', 'kubeadm' and 'iproute-tc' on master node 
- name: Installing Docker & kubeadm on Master Node
  package:
    name:
      - kubeadm
      - iproute2
    state: present

# Starting and enabling the docker on master node.
- name: enabling Docker on Master Node
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: ensure docker registry is available
  apt_repository: repo='deb https://download.docker.com/linux/ubuntu bionic stable' state=present

- name: ensure docker and dependencies are installed
  apt: name=docker-ce update_cache=yes

- name: ensure docker-ce-cli installed
  apt: name=docker-ce-cli

- name: ensure containerd.io installed
  apt: name=containerd.io

# Starting kubelet on master node. we maybe have a question on why kubelet is run on master node?
# The reason for running kubelet on master node is that, kubeadm uses containers (pods) to deploy etcd and the api server components. 
# For this, static manifests are created as yaml-files which are picked up by the kubelet on the master node 
# to provide the infrastructure pods. An added benefit is that you have the master node metrics available from the kubernetes api.
- name: Staring kubelet on Master Node
  service:
    name: kubelet
    state: started
    enabled: yes

# Updating Docker cgroup on Master Node. Reason for using cgroupdriver as 'systemd' instead of default ones, 
# https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/
- name: Updating Docker cgroup on Master Node
  copy:
    dest: /etc/docker/daemon.json
    content: |
      {
      "exec-opts": ["native.cgroupdriver=systemd"]
      }

# Restart the docker on Master node to get all things activated
- name: Restart docker on Master Node
  service:
    name: docker
    state: restarted

- name: swapp off
  shell: swapoff -a

# Updating IP tables on Master Node. Usually in the linux distros like ubuntu,
# (https://www.kevinhooke.com/2018/11/23/checking-iptables-filtering-for-bridge-networking-on-ubuntu-for-kubernetes-setup/)
# the bridge module in the kernel for "ip6tables, iptables and arptables" has the default set to "1"
# ("on", i.e. "*do* send the packets to iptables"), but for the distros like Fedora, RHEL, and CentOS they are set to '0' so we need to
# update the '/etc/sysctl.d/k8s.conf' to set it to '1'
- name: Updating IP tables on master Node
  copy:
    dest: /etc/sysctl.d/k8s.conf
    content: |
      net.bridge.bridge-nf-call-ip6tables = 1
      net.bridge.bridge-nf-call-iptables = 1

# Reloading sysctl on Slave Node to activate the bridge changes
- name: Reloading sysctl on master Node
  command: sysctl --system

- name: Workaround for running kube-api server on ubuntu_21.x and later versions
  shell:
    cmd: |
      containerd config default | tee /etc/containerd/config.toml
      sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
      service containerd restart
      service kubelet restart

- name: Get ZeroTier tunnel ip
  shell: ip -4 -o a | cut -d ' ' -f 2,7 | cut -d '/' -f 1 | grep '^zt' | cut -d " " -f2
  register: tunnel_ip

# Initialize kubernetes cluster with a pod network so that the Pods can talk to each other, also ignoring errors like 'NumCPU', 'Mem'
# and 'FileContent--proc-sys-net-bridge-bridge-nf-call-iptables' as they are limited to system resources.
- name: initialize the cluster
  shell: "kubeadm init --apiserver-advertise-address {{ tunnel_ip.stdout }} --pod-network-cidr=192.168.0.0/16"
  args:
    chdir: $HOME
  register: token_log
- debug:
      msg: "{{ token_log.stdout }}"

# Setting kubectl on master node with updating kubernetes configuration from default path to '$HOME/.kube/config' path and updating
# ownership permissions to user and group.
- name: create a directory for kube configs
  shell: mkdir -p $HOME/.kube

- name: copy config file to new directory
  shell: cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

- name: Set permissions for the file
  shell: chown $(id -u):$(id -g) $HOME/.kube/config

# Only required this if the user is root
# - name: export admin config
#   shell: export KUBECONFIG=/etc/kubernetes/admin.conf

- name: Pause for 1 minute to run calico
  ansible.builtin.pause:
    minutes: 1

# Deploying a CNI plugin on master node, we use 'calico' for it, https://projectcalico.docs.tigera.io/about/about-calico
- name: install Pod network
  shell: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
  args:
    chdir: $HOME
  register: calico_debug
- debug:
      msg: "{{ calico_debug.stdout }}"

# Configure client ovpn
- name: "copy deployments file to master"
  template:
    src: deployments.yml
    dest: "/root/"

# Configure client ovpn
- name: "copy services file to master"
  template:
    src: services.yml
    dest: "/root/"

# copy nginx LB config to the server
- name: "copy nginx file to master"
  template:
    src: nginx.conf
    dest: "/root/"

# Cleaning caches on nodes RAM's. The “shell” module used here to clean the buffer cache on the master node,
# because while doing the setup it will create lots of temporary data on RAM
- name: Cleaning Caches on RAM
  shell: echo 3 > /proc/sys/vm/drop_caches