---
# tasks file for k8s_slave

# Adding kubernetes repo from base url using gpg keys
- name: install APT Transport HTTPS
  apt:
    name: apt-transport-https
    state: present
 
- name: add Kubernetes apt-key
  apt_key:
    url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
    state: present
 
- name: add Kubernetes' APT repository
  apt_repository:
    repo: deb http://apt.kubernetes.io/ kubernetes-xenial main
    state: present
    filename: 'kubernetes'

# Installing 'docker', 'kubeadm' and 'iproute-tc' on slave node 
- name: Installing Docker & kubeadm on Slave Node
  package:
    name:
      - kubeadm
      - iproute2
    state: present

# Starting and enabling the docker on master node.
- name: enabling Docker on Master Node
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: ensure docker registry is available
  apt_repository: repo='deb https://download.docker.com/linux/ubuntu bionic stable' state=present

- name: ensure docker and dependencies are installed
  apt: name=docker-ce update_cache=yes
  
# Starting kubelet services on slave node.
- name: Staring & enabling Docker & kubelet on Slave Node
  service:
    name: kubelet
    state: started
    enabled: yes

# Updating Docker cgroup on slave Node. Reason for using cgroupdriver as 'systemd' instead of default ones, 
# https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/
- name: Updating Docker cgroup on Slave Node
  copy:
    dest: /etc/docker/daemon.json
    content: |
      {
      "exec-opts": ["native.cgroupdriver=systemd"]
      }

# Restart the docker on slave node to get all things activated
- name: Restart Docker on Slave Node
  service:
    name: docker
    state: restarted

# Set the hostname as FQDN 
- name: Set the hostname as FQDN 
  shell: hostnamectl set-hostname $(curl http://169.254.169.254/latest/meta-data/local-hostname)

# Check hostname
- name: Check hostname
  shell: hostname
  register: hostname_output
- debug:
    var: hostname_output.stdout_lines

# Create a /etc/kubernetes/aws.yml file
# - name: Create a /etc/kubernetes/node(1-3).yml file(s)
#   shell: mkdir -p /etc/kubernetes/node1.yml /etc/kubernetes/node2.yml /etc/kubernetes/node3.yml

# Copy node.yml templates to /etc/kubernetes/node.yml file
#- name: Copy node.yml templates to /etc/kubernetes/node.yml file
#  copy: src={{ item.src }} dest={{ item.dest }}
#  with_items:
#    - { src: 'roles/k8s_slave/templates/node.yml.j2', dest: '/etc/kubernetes/node.yml' }

# Updating IP tables on Slave Node. Usually in the linux distros like ubuntu,
# (https://www.kevinhooke.com/2018/11/23/checking-iptables-filtering-for-bridge-networking-on-ubuntu-for-kubernetes-setup/)
# the bridge module in the kernel for "ip6tables, iptables and arptables" has the default set to "1"
# ("on", i.e. "*do* send the packets to iptables"), but for the distros like Fedora, RHEL, and CentOS they are set to '0' so we need to
# update the '/etc/sysctl.d/k8s.conf' to set it to '1'
- name: Updating IP tables on Slave Node
  copy:
    dest: /etc/sysctl.d/k8s.conf
    content: |
      net.bridge.bridge-nf-call-ip6tables = 1
      net.bridge.bridge-nf-call-iptables = 1

# Reloading sysctl on Slave Node to activate the bridge changes
- name: Reloading sysctl on Slave Node
  command: sysctl --system

# Now this is the crucial part, joining our slave nodes to our master node 
# “hostvars[groups[‘ec2_master’][0]]” option is calling the namespace of the master node from ec2 role.
# Next, using “[‘token’][‘stdout’]” we just parse the command that we can use in slave to join the master node
# - name: Joining the master node
#   command: "{{ hostvars[groups['ec2_master'][0]]['token']['stdout'] }}"

- name: install wireguard client
  apt:
    name: wireguard
    state: present
    update_cache: yes

- name: generate public and private key
  shell:
    cmd: umask 077 && wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey

- name: create client wireguard config
  template:
    dest: /etc/wireguard/wg0.conf
    src: roles/k8s_slave_wg/templates/wg0.conf.j2
    owner: root
    group: root
    mode: '0600'

# Cleaning caches on nodes RAM's. The “shell” module used here to clean the buffer cache on the slave nodes,
# because while doing the setup it will create lots of temporary data on RAM
- name: Cleaning Caches on RAM
  shell: echo 3 > /proc/sys/vm/drop_caches